{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12d3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pywhispercpp.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70cefed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/quentin/dev/podcast_audio_extractor/whisper.cpp/models/ggml-large-v3.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51866\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 128\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5 (large v3)\n",
      "whisper_model_load: adding 1609 extra tokens\n",
      "whisper_model_load: n_langs       = 100\n",
      "whisper_default_buffer_type: using device Metal (Apple M2)\n",
      "whisper_model_load:    Metal total size =  3094.36 MB\n",
      "whisper_model_load: model size    = 3094.36 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12713.12 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   83.89 MB\n",
      "whisper_init_state: kv cross size =  251.66 MB\n",
      "whisper_init_state: kv pad  size  =    7.86 MB\n",
      "whisper_init_state: loading Core ML model from '/Users/quentin/dev/podcast_audio_extractor/whisper.cpp/models/ggml-large-v3-encoder.mlmodelc'\n",
      "whisper_init_state: first run on a device may take a while ...\n",
      "whisper_init_state: Core ML model loaded\n",
      "whisper_init_state: compute buffer (conv)   =   10.79 MB\n",
      "whisper_init_state: compute buffer (cross)  =   16.93 MB\n",
      "whisper_init_state: compute buffer (decode) =  100.03 MB\n"
     ]
    }
   ],
   "source": [
    "model = Model(model='large-v3', models_dir='./whisper.cpp/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd411ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './s10e43_tiny_benchmark.mp3'\n",
    "\n",
    "def gathering_files_in_directory() -> list[str]:\n",
    "    print('test')\n",
    "\n",
    "def transcribe(file:str):\n",
    "    transcription = model.transcribe(\n",
    "        file, \n",
    "        language='fr',\n",
    "        temperature=0.0,\n",
    "        print_progress=True,\n",
    "        # print_confidence=False\n",
    "    )\n",
    "    print(transcription)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b769537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_into_file(file:str):\n",
    "\n",
    "    # directory to store the transcriptions into \n",
    "    output_directory = 'transcriptions_output'\n",
    "    os.makedirs(f'./{output_directory}', exist_ok=True)\n",
    "\n",
    "    # name of the transcription output file\n",
    "    # (using the name of the file given as params)\n",
    "    base_name = os.path.splitext(file)[0]\n",
    "    output_file_name = f\"{base_name}.txt\"\n",
    "\n",
    "    # writing transcriptions in file\n",
    "    with open(f'./{output_directory}/{output_file_name}','w',encoding='utf-8') as output_file:\n",
    "        for segment in transcribe(file):\n",
    "            output_file.write(f'{segment.text}')\n",
    "            # print(segment)\n",
    "\n",
    "\n",
    "def format_output():\n",
    "    print('format')\n",
    "\n",
    "def store_into_db():\n",
    "    print('db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e2eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%\n",
      "Progress:  36%\n",
      "Progress:  78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t0=0, t1=140, text=Message publicitaire., probability=nan, t0=140, t1=1022, text=Le médicament Daflon 500 mg, indiqué pour soulager les jambes lourdes et douloureuses dues à l'insuffisance veineuse, agit en renforçant le tonus veineux et en protégeant les vaisseaux sanguins., probability=nan, t0=1022, t1=1874, text=Son efficacité a été cliniquement démontrée dans le traitement des troubles de la circulation veineuse, jambes lourdes, douleurs, impatience, en complément des mesures hygiéno-diététiques., probability=nan, t0=1874, t1=2034, text=Rendez-vous sur Daflon.fr., probability=nan, t0=2034, t1=2624, text=Daflon 500 mg, composé de fractions flavonoïques purifiées micronisées et réservées à l'adulte, est disponible en pharmacie sans ordonnance., probability=nan, t0=2624, t1=3006, text=Tout médicament peut exposer à des risques. Parlez-en à votre pharmacien et lisez attentivement la notice., probability=nan, t0=3006, t1=3230, text=Si les symptômes persistent, consultez votre médecin., probability=nan, t0=5624, t1=6090, text=Ah non, je suis là. Vous qui n'avez pas d'invité pour nous mettre la honte de cette intro., probability=nan, t0=6090, t1=6388, text=Bonjour, bonsoir et bienvenue dans ce nouvel épisode et dernier., probability=nan, t0=6388, t1=6560, text=Ah, je le dis quand même., probability=nan, t0=6560, t1=7028, text=Parce que je me disais, est-ce qu'il va faire le truc de bienvenue dans ce dernier épisode du podcast ?, probability=nan, t0=7028, t1=7164, text=Ah non, je le fais à l'arrache quand même., probability=nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "transcribe_into_file(file)\n",
    "\n",
    "# transcribe(file)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67afd087",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# unload model from vram\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# unload model from vram\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
