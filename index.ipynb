{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywhispercpp.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cefed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/home/quentin/dev/podcast_transcription/whisper.cpp/models/ggml-large-v3.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_init_with_params_no_state: devices    = 2\n",
      "whisper_init_with_params_no_state: backends   = 2\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51866\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 128\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5 (large v3)\n",
      "whisper_model_load: adding 1609 extra tokens\n",
      "whisper_model_load: n_langs       = 100\n",
      "whisper_default_buffer_type: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU)\n",
      "whisper_model_load:    CUDA0 total size =  3094.36 MB\n",
      "whisper_model_load: model size    = 3094.36 MB\n",
      "whisper_backend_init_gpu: using CUDA0 backend\n",
      "whisper_init_state: kv self size  =   83.89 MB\n",
      "whisper_init_state: kv cross size =  251.66 MB\n",
      "whisper_init_state: kv pad  size  =    7.86 MB\n",
      "whisper_init_state: compute buffer (conv)   =   37.67 MB\n",
      "whisper_init_state: compute buffer (encode) =  212.29 MB\n",
      "whisper_init_state: compute buffer (cross)  =    9.25 MB\n",
      "whisper_init_state: compute buffer (decode) =  100.03 MB\n",
      "Progress:   0%\n",
      "Progress:   4%\n",
      "Progress:   9%\n",
      "Progress:  14%\n",
      "Progress:  19%\n",
      "Progress:  24%\n",
      "Progress:  29%\n",
      "Progress:  34%\n",
      "Progress:  39%\n",
      "Progress:  44%\n",
      "Progress:  49%\n",
      "Progress:  54%\n",
      "Progress:  59%\n",
      "Progress:  64%\n",
      "Progress:  69%\n",
      "Progress:  73%\n",
      "Progress:  78%\n",
      "Progress:  82%\n",
      "Progress:  87%\n",
      "Progress:  92%\n",
      "Progress:  97%\n",
      "Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "model = Model(model='large-v3', models_dir='./whisper.cpp/models')\n",
    "segments = model.transcribe('./s10e43_trimmed_benchmark.mp3', )\n",
    "\n",
    "# for segment in segments:\n",
    "#     print(segment.text)\n",
    "\n",
    "# print(model.system_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
