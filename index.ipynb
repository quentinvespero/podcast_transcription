{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pywhispercpp.model import Model\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from yt_dlp import YoutubeDL\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45016206",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9e08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'data/podcast.db'\n",
    "\n",
    "input_files_dir = Path('input_files/')\n",
    "# print(input_files_dir.resolve())\n",
    "# print(list(input_files_dir.glob('*.mp3')))\n",
    "file = './s10e43_tiny_benchmark.mp3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119a434",
   "metadata": {},
   "source": [
    "### database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a32e9",
   "metadata": {},
   "source": [
    "##### init database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9488635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder data/ if doesn't exist yet\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# create/connect to sqlite database\n",
    "def get_connection():\n",
    "    return sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create db structure\n",
    "def init_db():\n",
    "    with get_connection() as db:\n",
    "        cursor = db.cursor()\n",
    "\n",
    "        # episodes table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS episodes (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                date TEXT,\n",
    "                url_path TEXT,\n",
    "                description TEXT,\n",
    "                season_number INTEGER,\n",
    "                episode_number INTEGER,\n",
    "                index_number INTEGER\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # participants table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS participants (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                name TEXT NOT NULL\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # transcript segments table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS transcription_segments (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                episode_id INTEGER REFERENCES episodes(id),\n",
    "                start_time REAL,\n",
    "                end_time REAL,\n",
    "                text TEXT,\n",
    "                participant_id INTEGER REFERENCES participants(id) -- speaker for diarisation\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # --- junction table for a participant in an episode\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS episodes_participants (\n",
    "                episode_id INTEGER REFERENCES episodes(id),\n",
    "                participant_id INTEGER REFERENCES participants(id),\n",
    "                role TEXT,\n",
    "                PRIMARY KEY (episode_id, participant_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        db.commit()\n",
    "\n",
    "# init the db\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95283dc2",
   "metadata": {},
   "source": [
    "##### check the structure of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d007b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "episodes\n",
      "(0, 'id', 'INTEGER', 0, None, 1)\n",
      "(1, 'title', 'TEXT', 0, None, 0)\n",
      "(2, 'date', 'TEXT', 0, None, 0)\n",
      "(3, 'url_path', 'TEXT', 0, None, 0)\n",
      "(4, 'description', 'TEXT', 0, None, 0)\n",
      "(5, 'season_number', 'INTEGER', 0, None, 0)\n",
      "(6, 'episode_number', 'INTEGER', 0, None, 0)\n",
      "(7, 'index_number', 'INTEGER', 0, None, 0)\n",
      "\n",
      "participants\n",
      "(0, 'id', 'INTEGER', 0, None, 1)\n",
      "(1, 'name', 'TEXT', 1, None, 0)\n",
      "\n",
      "transcription_segments\n",
      "(0, 'id', 'INTEGER', 0, None, 1)\n",
      "(1, 'episode_id', 'INTEGER', 0, None, 0)\n",
      "(2, 'start_time', 'REAL', 0, None, 0)\n",
      "(3, 'end_time', 'REAL', 0, None, 0)\n",
      "(4, 'text', 'TEXT', 0, None, 0)\n",
      "(5, 'participant_id', 'INTEGER', 0, None, 0)\n",
      "\n",
      "episodes_participants\n",
      "(0, 'episode_id', 'INTEGER', 0, None, 1)\n",
      "(1, 'participant_id', 'INTEGER', 0, None, 2)\n",
      "(2, 'role', 'TEXT', 0, None, 0)\n"
     ]
    }
   ],
   "source": [
    "def inspect_table_structure(table_name):\n",
    "    \"\"\"Inspect and print table structure\"\"\"\n",
    "    with get_connection() as db:\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(f'PRAGMA table_info({table_name})')\n",
    "        print(f'\\n{table_name}')\n",
    "        for col in cursor.fetchall():\n",
    "            print(col)\n",
    "\n",
    "# Usage\n",
    "tables = ['episodes', 'participants', 'transcription_segments', 'episodes_participants']\n",
    "for table in tables:\n",
    "    inspect_table_structure(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b26580",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cefed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/quentin/dev/podcast_audio_extractor/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_default_buffer_type: using device Metal (Apple M2)\n",
      "whisper_model_load:    Metal total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12713.12 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =    6.29 MB\n",
      "whisper_init_state: kv cross size =   18.87 MB\n",
      "whisper_init_state: kv pad  size  =    3.15 MB\n",
      "whisper_init_state: loading Core ML model from '/Users/quentin/dev/podcast_audio_extractor/whisper.cpp/models/ggml-base.en-encoder.mlmodelc'\n",
      "whisper_init_state: first run on a device may take a while ...\n",
      "whisper_init_state: Core ML model loaded\n",
      "whisper_init_state: compute buffer (conv)   =    5.61 MB\n",
      "whisper_init_state: compute buffer (cross)  =    7.72 MB\n",
      "whisper_init_state: compute buffer (decode) =   97.27 MB\n"
     ]
    }
   ],
   "source": [
    "# model = Model(model='large-v3', models_dir='./whisper.cpp/models')\n",
    "model = Model(model='base.en', models_dir='./whisper.cpp/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d1a0b",
   "metadata": {},
   "source": [
    "### process episode - insert episode into sqlite db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156174bc",
   "metadata": {},
   "source": [
    "##### gathering episode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b57c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the season number as well as the episode number from the title string\n",
    "# the title string should be formated as such 'S01E01...'\n",
    "# it will then return season and episode as a number\n",
    "def extract_season_episode(episode_title:str):\n",
    "    if episode_title is None:\n",
    "        return None, None\n",
    "\n",
    "    pattern_to_find = r'S(\\d+)E(\\d+)'\n",
    "    match = re.search(pattern_to_find, episode_title)\n",
    "\n",
    "    if match:\n",
    "        season_number = int(match.group(1))\n",
    "        episode_number = int(match.group(2))\n",
    "    else:\n",
    "        season_number = None\n",
    "        episode_number = None\n",
    "\n",
    "    return season_number, episode_number\n",
    "\n",
    "# function to convert the index number to reverse\n",
    "# def convert_index_number():\n",
    "#     print('convert index')\n",
    "\n",
    "# getting episode data\n",
    "def fetch_episodes_data(feed_url, episode_items):\n",
    "    ydl_config = {\n",
    "        'extract_flat': False,\n",
    "        'playlist_items': episode_items,\n",
    "        'quiet': True,\n",
    "        'skip_download': True\n",
    "    }\n",
    "    with YoutubeDL(ydl_config) as ydl:\n",
    "        info = ydl.extract_info(feed_url) # gather data from the feed url\n",
    "        episodes = info.get('entries', [info]) # make up a list of items from the fetched entries\n",
    "        return [\n",
    "            {\n",
    "                \"title\": episode.get(\"title\"),\n",
    "                \"description\": episode.get(\"description\"),\n",
    "                \"url\": episode.get(\"webpage_url\") or episode.get(\"original_url\"),\n",
    "                \"upload_date\": episode.get(\"upload_date\"),\n",
    "                \"playlist_index\": episode.get(\"playlist_index\"),\n",
    "                \"season_number\": season,\n",
    "                \"episode_number\": episode_num,\n",
    "            }\n",
    "            for episode in episodes\n",
    "            for season, episode_num in [extract_season_episode(episode.get('title'))]\n",
    "        ]\n",
    "\n",
    "# download episode\n",
    "def download_episode(episode_url, download_path, episode_title):\n",
    "    ydl_config = {\n",
    "        'outtmpl': f\"{download_path}/{episode_title}.%(ext)s\",\n",
    "        # 'format':'',\n",
    "        'quiet': True\n",
    "    }\n",
    "    with YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([episode_url])\n",
    "    print('downloaded episode with title :', {episode_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "930f5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [generic] Falling back on generic information extractor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'S02E02 - Postiche de Fouffe',\n",
       "  'description': '<p>Avec Maud Givert, Sophie Riche et Sophie-Marie Larrouy.</p><br><p>Présenté par Florent Bernard et Adrien Ménielle.</p><br><p>Dans ce podcast, après le traditionnel tour de table de ce qu\\'on a kiffé récemment et une longue parenthèse sur le film \"Pattaya\", nous parlons de nos ratés estivaux, nos vacances gâchés, bref que c\\'était bien de la merde nos étés.</p><hr><p style=\\'color:grey; font-size:0.75em;\\'> Hébergé par Acast. Visitez <a style=\\'color:grey;\\' target=\\'_blank\\' rel=\\'noopener noreferrer\\' href=\\'https://acast.com/privacy\\'>acast.com/privacy</a> pour plus d\\'informations.</p>',\n",
       "  'url': 'https://sphinx.acast.com/p/open/s/5ffe3facad3e633276e9ea57/e/tag%3Asoundcloud%2C2010%3Atracks%2F285183974/media.mp3#__youtubedl_smuggle=%7B%22force_videoid%22%3A+%22tag%3Asoundcloud%2C2010%3Atracks%2F285183974%22%7D',\n",
       "  'upload_date': '20160928',\n",
       "  'playlist_index': 240,\n",
       "  'season_number': 2,\n",
       "  'episode_number': 2},\n",
       " {'title': 'S02E01 - Live @ Frames 2016',\n",
       "  'description': \"Avec Patrick Baud, Charlie Danger, Slimane Baptiste Berhoun, Thomas Hercouet, François Descraques et Adrien MénielleNouvelle saison de Floodcast avec pour fêter ça un épisode EN PUBLIC au Frames Festival 2016 ! Dans cette émission, on parle du fait de sortir de sa zone de confort, cet endroit douillet et chaud mais ennuyeux et triste si on reste trop longtemps dedans.C'était mon premier épisode en public, désolé pour les petits bugs de son et le ton parfois un peu moins naturel que dans mon salon, mais c'est aussi ça sortir de sa zone de confort : ça fait flipper.<hr><p style='color:grey; font-size:0.75em;'> Hébergé par Acast. Visitez <a style='color:grey;' target='_blank' rel='noopener noreferrer' href='https://acast.com/privacy'>acast.com/privacy</a> pour plus d'informations.</p>\",\n",
       "  'url': 'https://sphinx.acast.com/p/open/s/5ffe3facad3e633276e9ea57/e/tag%3Asoundcloud%2C2010%3Atracks%2F282640643/media.mp3#__youtubedl_smuggle=%7B%22force_videoid%22%3A+%22tag%3Asoundcloud%2C2010%3Atracks%2F282640643%22%7D',\n",
       "  'upload_date': '20160913',\n",
       "  'playlist_index': 241,\n",
       "  'season_number': 2,\n",
       "  'episode_number': 1},\n",
       " {'title': 'S01E09 - Oh ça va...',\n",
       "  'description': \"<p>Avec Anne-Sophie Girard, Simon Astier et Nicolas Berno.</p><br><p>Présenté par Florent Bernard et Adrien Ménielle. </p><br><p> Dans cette émission, nous abordons un sujet à priori simple: qu'est-ce qui différence un connard de quelqu'un de bien ? La bouffonne de la meuf sympa ? La baltringue du gentil ?</p><hr><p style='color:grey; font-size:0.75em;'> Hébergé par Acast. Visitez <a style='color:grey;' target='_blank' rel='noopener noreferrer' href='https://acast.com/privacy'>acast.com/privacy</a> pour plus d'informations.</p>\",\n",
       "  'url': 'https://sphinx.acast.com/p/open/s/5ffe3facad3e633276e9ea57/e/tag%3Asoundcloud%2C2010%3Atracks%2F260424472/media.mp3#__youtubedl_smuggle=%7B%22force_videoid%22%3A+%22tag%3Asoundcloud%2C2010%3Atracks%2F260424472%22%7D',\n",
       "  'upload_date': '20160424',\n",
       "  'playlist_index': 242,\n",
       "  'season_number': 1,\n",
       "  'episode_number': 9}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage test\n",
    "fetch_episodes_data('https://feeds.acast.com/public/shows/floodcast', '240-242')\n",
    "# fetch_episodes_data('https://feeds.acast.com/public/shows/floodcast', '240-250')\n",
    "# test_url_episode = 'https://sphinx.acast.com/p/open/s/5ffe3facad3e633276e9ea57/e/tag%3Asoundcloud%2C2010%3Atracks%2F285183974/media.mp3#__youtubedl_smuggle=%7B%22force_videoid%22%3A+%22tag%3Asoundcloud%2C2010%3Atracks%2F285183974%22%7D'\n",
    "# download_episode(test_url_episode, './output', 'panchour')\n",
    "# extract_season_episode('S22E46 - Postiche de Fouffe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409c833",
   "metadata": {},
   "source": [
    "##### create episode into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4547142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert episode into db\n",
    "def create_episode_in_db(episode_data):\n",
    "    try:\n",
    "        with get_connection() as db:\n",
    "            cursor = db.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO episodes (title, date, url_path, description, season_number, episode_number, index_number)\n",
    "                VALUES (?,?,?,?,?,?,?)\n",
    "            \"\"\", (\n",
    "                episode_data.get('title'),\n",
    "                episode_data.get('upload_date'),\n",
    "                episode_data.get('url'),\n",
    "                episode_data.get('description'),\n",
    "                episode_data.get('season_number'),\n",
    "                episode_data.get('episode_number'),\n",
    "                episode_data.get('playlist_index')\n",
    "            ))\n",
    "            \n",
    "            db.commit()\n",
    "            episode_id = cursor.lastrowid\n",
    "            print('episode created in db with id :',{episode_id})\n",
    "\n",
    "        return episode_id\n",
    "    except Exception as e:\n",
    "        print(f'Error creating episode in DB: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc09207",
   "metadata": {},
   "source": [
    "### process episode - create episode transcriptions & store segments into sqlite db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c02af",
   "metadata": {},
   "source": [
    "##### transcription function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a9116",
   "metadata": {},
   "source": [
    "for testing whispercpp parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eef854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file:str):\n",
    "    transcription = model.transcribe(\n",
    "        file, \n",
    "        language='fr',\n",
    "        temperature=0.0,\n",
    "        print_progress=True,\n",
    "        extract_probability=False\n",
    "    )\n",
    "    print(transcription)\n",
    "    return transcription\n",
    "\n",
    "# help(Model.transcribe)\n",
    "# ?Model.transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c93dd9",
   "metadata": {},
   "source": [
    "##### clean transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1e4c6",
   "metadata": {},
   "source": [
    "##### store transcription segments into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3971825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print la transcription dans un fichier\n",
    "# def transcribe_into_file(file:str):\n",
    "\n",
    "#     # directory to store the transcriptions into \n",
    "#     output_directory = 'transcriptions_output'\n",
    "#     os.makedirs(f'./{output_directory}', exist_ok=True)\n",
    "\n",
    "#     # name of the transcription output file\n",
    "#     # (using the name of the file given as params)\n",
    "#     base_name = os.path.splitext(file)[0]\n",
    "#     output_file_name = f\"{base_name}.txt\"\n",
    "\n",
    "#     # writing transcriptions in file\n",
    "#     with open(f'./{output_directory}/{output_file_name}','w',encoding='utf-8') as output_file:\n",
    "#         for segment in transcribe(file):\n",
    "#             output_file.write(f'{segment.text}')\n",
    "#             # print(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b769537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe_into_db(input_files_dir):\n",
    "    \n",
    "#     # for each files in the input audio files folder, it get the transcription, \n",
    "#     # and then stores it in the db\n",
    "#     for audio_file in input_files_dir.glob('*.mp3'):\n",
    "\n",
    "#         print(f\"file being processed : {audio_file.name}\")\n",
    "        \n",
    "#         # create episode and get its id\n",
    "#         episode_id = create_episode_in_db(audio_file)\n",
    "\n",
    "#         # store transcription segment of the episode into db\n",
    "#         with get_connection() as db:\n",
    "#             cursor = db.cursor()\n",
    "\n",
    "#             for transcription_segment in transcribe(audio_file.name):\n",
    "#                 start = transcription_segment.t0\n",
    "#                 end = transcription_segment.t1\n",
    "#                 text = transcription_segment.text.strip()\n",
    "\n",
    "#                 cursor.execute(\"\"\"\n",
    "#                     INSERT INTO transcription_segments (episode_id, start_time, end_time, text)\n",
    "#                     VALUES (?,?,?,?)\n",
    "#                 \"\"\", (episode_num, start, end, text))\n",
    "\n",
    "#             db.commit()\n",
    "\n",
    "# store transcription segment of the episode in db\n",
    "def store_transcripts_in_db(transcript_segments, episode_id):\n",
    "    with get_connection() as db:\n",
    "        cursor = db.cursor()\n",
    "\n",
    "        for transcript_segment in transcript_segments:\n",
    "            start = transcript_segment.t0\n",
    "            end = transcript_segment.t1\n",
    "            text = transcript_segment.text.strip()\n",
    "\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO transcription_segments (episode_id, start_time, end_time, text)\n",
    "                VALUES (?,?,?,?)\n",
    "            \"\"\", (episode_id, start, end, text))\n",
    "\n",
    "        db.commit()\n",
    "\n",
    "    # print('transcripts stored for episode with id :',{episode_id})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578a4b",
   "metadata": {},
   "source": [
    "### process episode - create embeddings & store into vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc27be",
   "metadata": {},
   "source": [
    "##### create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8c7f8",
   "metadata": {},
   "source": [
    "##### store embeding into vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9ea8d",
   "metadata": {},
   "source": [
    "### main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c21aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - get the data of the episode(s)\n",
    "# - download the audio file\n",
    "# - create the episode in the db\n",
    "def process_episodes(feed_url, episodes_items, download_path='./files_to_transcribe/', download=False, transcription=False):\n",
    "    episodes = fetch_episodes_data(feed_url, episodes_items)\n",
    "\n",
    "    for episode in episodes:\n",
    "\n",
    "        # get episode title\n",
    "        episode_title = episode['title']\n",
    "        print('processing episode :', episode_title)\n",
    "        \n",
    "        # create episode in db\n",
    "        episode_id = create_episode_in_db(episode)\n",
    "        print('episode id in db :',episode_id)\n",
    "\n",
    "        # download the episode file\n",
    "        if download:\n",
    "            episode_file_path = download_episode(episode['url'],download_path, episode_title)\n",
    "\n",
    "        # transcribe the episode file and store its transcription segments in the db\n",
    "        if download and episode_file_path and transcription:\n",
    "            # getting the transcription segments generated by whisper\n",
    "            transcript_segments = transcribe(episode_file_path)\n",
    "            # storing the gathered segments and store them in db\n",
    "            store_transcripts_in_db(transcript_segments, episode_id)\n",
    "\n",
    "        if transcription and not download:\n",
    "            print('download must be true in order to be able to make transcriptions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d63361",
   "metadata": {},
   "source": [
    "### usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5b738",
   "metadata": {},
   "source": [
    "parameters to provide :\n",
    "- feed url\n",
    "- episode to get (example : '1' for the episode 1, '1-200' for a range from episode 1 to 200)\n",
    "- (optional) download path for the audio files (if not given, no episode downloaded)\n",
    "- (optional) transcription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [generic] Falling back on generic information extractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing episode : S10E40 - Le Cri du Jugnot\n",
      "episode created in db with id : {10}\n",
      "episode id in db : 10\n",
      "download must be true in order to be able to make transcriptions\n"
     ]
    }
   ],
   "source": [
    "process_episodes(\n",
    "    feed_url='https://feeds.acast.com/public/shows/floodcast',\n",
    "    episodes_items='4',\n",
    "    # download_path='',\n",
    "    transcription=True,\n",
    "    download=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
