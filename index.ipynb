{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d3021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pywhispercpp.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cefed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/home/quentin/dev/podcast_transcription/whisper.cpp/models/ggml-large-v3.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_init_with_params_no_state: devices    = 2\n",
      "whisper_init_with_params_no_state: backends   = 2\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51866\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 128\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5 (large v3)\n",
      "whisper_model_load: adding 1609 extra tokens\n",
      "whisper_model_load: n_langs       = 100\n",
      "whisper_default_buffer_type: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU)\n",
      "whisper_model_load:    CUDA0 total size =  3094.36 MB\n",
      "whisper_model_load: model size    = 3094.36 MB\n",
      "whisper_backend_init_gpu: using CUDA0 backend\n",
      "whisper_init_state: kv self size  =   83.89 MB\n",
      "whisper_init_state: kv cross size =  251.66 MB\n",
      "whisper_init_state: kv pad  size  =    7.86 MB\n",
      "whisper_init_state: compute buffer (conv)   =   37.67 MB\n",
      "whisper_init_state: compute buffer (encode) =  212.29 MB\n",
      "whisper_init_state: compute buffer (cross)  =    9.25 MB\n",
      "whisper_init_state: compute buffer (decode) =  100.03 MB\n"
     ]
    }
   ],
   "source": [
    "model = Model(model='large-v3', models_dir='./whisper.cpp/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd411ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './s10e43_evenMoreTrimmed_benchmark.mp3'\n",
    "\n",
    "def gathering_files_in_directory() -> list[str]:\n",
    "    print('test')\n",
    "\n",
    "def transcribe(file:str):\n",
    "    transcription = model.transcribe(\n",
    "        file, \n",
    "        language='fr',\n",
    "        temperature=0.0,\n",
    "        print_progress=True,\n",
    "        # print_confidence=True\n",
    "    )\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b769537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_into_file(file:str):\n",
    "\n",
    "    # directory to store the transcriptions into \n",
    "    output_directory = 'transcriptions_output'\n",
    "    os.makedirs(f'./{output_directory}', exist_ok=True)\n",
    "\n",
    "    # name of the transcription output file\n",
    "    # (using the name of the file given as params)\n",
    "    base_name = os.path.splitext(file)[0]\n",
    "    output_name = f\"{base_name}.txt\"\n",
    "\n",
    "    # writing transcriptions in file\n",
    "    with open(f'./{output_directory}/{output_name}','w',encoding='utf-8') as output_file:\n",
    "        for segment in transcribe(file):\n",
    "            output_file.write(f'{segment}')\n",
    "            print(segment)\n",
    "\n",
    "\n",
    "def format_output():\n",
    "    print('format')\n",
    "\n",
    "def store_into_db():\n",
    "    print('db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e2eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%\n",
      "Progress:  18%\n",
      "Progress:  38%\n",
      "Progress:  59%\n",
      "Progress:  80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t0=0, t1=140, text=Message publicitaire.\n",
      "t0=140, t1=1022, text=Le médicament Daflon 500 mg, indiqué pour soulager les jambes lourdes et douloureuses dues à l'insuffisance veineuse, agit en renforçant le tonus veineux et en protégeant les vaisseaux sanguins.\n",
      "t0=1022, t1=1872, text=Son efficacité a été cliniquement démontrée dans le traitement des troubles de la circulation veineuse, jambes lourdes, douleurs, impatience, en complément des mesures hygiéno-diététiques.\n",
      "t0=1872, t1=2034, text=Rendez-vous sur Daflon.fr.\n",
      "t0=2034, t1=2624, text=Daflon 500 mg, composé de fractions flavonoïques purifiées micronisées et réservées à l'adulte, est disponible en pharmacie sans ordonnance.\n",
      "t0=2624, t1=3006, text=Tout médicament peut exposer à des risques. Parlez-en à votre pharmacien et lisez attentivement la notice.\n",
      "t0=3006, t1=3230, text=Si les symptômes persistent, consultez votre médecin.\n",
      "t0=5624, t1=6090, text=Bonjour, je suis là. Vous qui n'avez pas d'invité pour nous mettre la honte de cette intro.\n",
      "t0=6090, t1=6390, text=Bonjour, bonsoir et bienvenue dans ce nouvel épisode et dernier.\n",
      "t0=6390, t1=6560, text=Ah, je le dis quand même.\n",
      "t0=6560, t1=7050, text=Parce que je me disais, est-ce qu'il va faire le truc de bienvenue dans ce dernier épisode du Floodcast ?\n",
      "t0=7050, t1=7104, text=Ah non, je le fais à la rage comme ça.\n",
      "t0=7104, t1=7272, text=Tu l'as fait, nouvel et dernier.\n",
      "t0=7272, t1=7500, text=Nouvel et dernier épisode du Floodcast.\n",
      "t0=7500, t1=7628, text=Chialé, chialé.\n",
      "t0=7628, t1=8590, text=On s'est dit, en gros c'est Adrien qui a eu l'idée, mais j'étais d'accord avec lui, de ne pas finir sur des lives parce qu'on sait que les lives ce n'est pas forcément les meilleurs qui ont été d'écoute, etc.\n",
      "t0=8624, t1=8800, text=Donc pourquoi pas faire un vrai petit au revoir.\n",
      "t0=8800, t1=9072, text=Voilà, un petit bilan calmement, on se remet en chagas ce temps.\n",
      "t0=9072, t1=9320, text=Comment vas-tu Adrien déjà ?\n",
      "t0=9320, t1=9396, text=Eh bien, ça va bien.\n",
      "t0=9396, t1=9684, text=Ça va ? Remis de tes émotions ?\n",
      "t0=9684, t1=9748, text=Oui.\n",
      "t0=9748, t1=9822, text=C'est trois grosses journées.\n",
      "t0=9822, t1=10526, text=D'ailleurs, je voulais dire aux gens que quand je dis que je vais être en dépression et tout, c'était une blague.\n",
      "t0=10526, t1=10904, text=Parce que vraiment tout le monde après m'a dit, alors déjà genre...\n",
      "t0=10904, t1=11264, text=Déjà, sympa de te faire, alors ? Alors tu déprimes ?\n",
      "t0=11264, t1=11622, text=Non, non, mais je sais qu'après le dernier live, dans la...\n",
      "t0=11624, t1=11824, text=À rue, il y a des gens qui sont venus demander des photos et tout.\n",
      "t0=11824, t1=12298, text=Et après, en partant, ils ont dit, bon et puis ça va aller, prends soin de toi et tout.\n",
      "t0=12298, t1=12492, text=Non, mais je ne vais pas me flinguer, c'est bon.\n",
      "t0=12492, t1=13208, text=Et en vrai, je ne sais pas toi, mais moi, je n'ai pas du tout eu même de tristesse, de mélancolie, rien.\n",
      "t0=13208, t1=13830, text=J'étais vraiment juste de la satisfaction d'avoir bien clôturé le truc.\n",
      "t0=13830, t1=13968, text=Comme on l'entendait.\n",
      "t0=13968, t1=14156, text=Avec bon entendeur même.\n",
      "t0=14156, t1=14400, text=Ça, on l'a coupé au montage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "transcribe_into_file(file)\n",
    "\n",
    "# transcribe(file)\n",
    "\n",
    "# unload model from vram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
